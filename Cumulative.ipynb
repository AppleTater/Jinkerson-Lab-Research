{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cumulative",
      "provenance": [],
      "collapsed_sections": [
        "g4fSvasvNasY",
        "Z3OvwxcvOZkW"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AppleTater/Jinkerson-Lab-Research/blob/master/Cumulative.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PMzurW8anTY"
      },
      "source": [
        "# Files that need to be uploaded\n",
        "\n",
        "\n",
        "---\n",
        "* TAGData.csv\n",
        "* large-lib_full_2016.10.25.csv\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4fSvasvNasY"
      },
      "source": [
        "# Importing Libraries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtvJ3gtHFRzi"
      },
      "source": [
        "**Python Libraries**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3_bkIj1jNbi"
      },
      "source": [
        "import pandas as pd                                       # importing common pandas libraries under the abbreviation 'pd' \n",
        "import matplotlib.pyplot as plt                           # was for the requested histogram under the abbreviation 'plt' \n",
        "\n",
        "import numpy.matlib                                       # to calculate mean and median\n",
        "import numpy as np                                        # numpy under the abbreviation 'np' \n",
        "from numpy import median                                  # from the numpy library, import how to get the median\n",
        "\n",
        "import scipy.stats as stats                               # libraries used for BH adjustment under the abbreviation 'stats' \n",
        "import scipy.special                                      # special libraries from scipy\n",
        "\n",
        "import re                                                 # more libraries\n",
        "import sys                                                # more libraries\n",
        "\n",
        "from IPython.display import (                             # display data, if wanted, as different media file types\n",
        "    display_pretty, display_html, display_jpeg,\n",
        "    display_png, display_json, display_latex, display_svg\n",
        ")\n",
        "\n",
        "from IPython.display import display                       # more display tools\n",
        "import itertools                                          # more display tools\n",
        "import gc                                                 # more display tools\n",
        "\n",
        "import statsmodels.api as sm                              # stats models under the abbreviation 'sm'\n",
        "import statsmodels.stats.multitest as smm                 # stats models under the abbreviation 'smm'\n",
        "import statsmodels.formula.api as smf                     # stats models under the abbreviation 'smf'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM0ZLceUFZGj"
      },
      "source": [
        "**R Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeFCVyiujV8p"
      },
      "source": [
        "%reload_ext rpy2.ipython\n",
        "\n",
        "# %%R\n",
        "# library(ggplot2)\n",
        "\n",
        "import rpy2.robjects as robjects                          # common rpy2 objects under the abbreviation 'robjects'\n",
        "from rpy2.robjects.packages import importr                # rpy2 packages under the abbreviation 'importr'\n",
        "from rpy2.robjects.vectors import FloatVector             # rpy2 for float conversion\n",
        "import gc                                                 # rpy2 library\n",
        "R_stats = importr('stats')                                # rpy2 stats\n",
        "\n",
        "from rpy2.rinterface_lib.sexp import StrSexpVector        # rpy2 interface\n",
        "from functools import partial                             # rpy2 tools under the abbreviation 'partial'\n",
        "from rpy2.ipython import html                             # html support\n",
        "html.html_rdataframe=partial(html.html_rdataframe, table_class=\"docutils\")\n",
        "\n",
        "from rpy2.robjects.conversion import localconverter       # rpy2 converter under the abbreviation 'localconverter'  \n",
        "\n",
        "import pandas as pd                                       # these libraries are for redundancy\n",
        "import rpy2.robjects as ro                                # â†“\n",
        "from rpy2.robjects.packages import importr                # â†“\n",
        "from rpy2.robjects import pandas2ri                       # â†“"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3OvwxcvOZkW"
      },
      "source": [
        "# Making of the Merged File\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNIt7fXu0jEg"
      },
      "source": [
        "data_df = pd.read_csv('TAG Data.csv')                           # read TAG Data.csv file, read in as 'data'\n",
        "\n",
        "data_df = data_df[['gene' , 'R5-S#11-HLG_WHOLE_normalized_reads' , 'R5-S#11-ALL_WHOLE_normalized_reads']]       \n",
        "      # create dataframe from the TAG Data using the included columns\n",
        "data_df ['Ratios: HLG / ALL (Whole)'] = data_df ['R5-S#11-HLG_WHOLE_normalized_reads'] / data_df['R5-S#11-ALL_WHOLE_normalized_reads']       \n",
        "      # create another column for the ratio, high lipid gate vs all\n",
        "\n",
        "data_df = data_df.sort_values('gene')                           # sort 'data' by the gene names, alphabetically (numerically in this case)\n",
        "data_df = data_df.replace([np.inf, -np.inf], np.nan)            # replace gene names that are 'np.inf' or '-np.inf' to 'np.nan'\n",
        "data_df = data_df.dropna()                                      # drop all rows that have 'np.nan' which we coverted in the step above\n",
        "\n",
        "data_df = data_df.drop(['R5-S#11-HLG_WHOLE_normalized_reads' , 'R5-S#11-ALL_WHOLE_normalized_reads'], axis = 1)\n",
        "      # drop these columns because the only use for them was to create the ratio, so they are no longer needed\n",
        "\n",
        "data_df = pd.DataFrame(data_df)\n",
        "      # consolidate all changes made to 'data' to a dataframe\n",
        "\n",
        "mean_df = data_df.groupby(['gene']).mean()                      # calculate the mean as an independent dataframe\n",
        "mean_df.rename(columns = {'Ratios: HLG / ALL (Whole)': 'Ratio Mean'}, inplace = True)\n",
        "      # rename column for better readbility\n",
        "\n",
        "median_df = data_df.groupby(['gene']).median()                  # find the median as an independent dataframe\n",
        "median_df.rename(columns = {'Ratios: HLG / ALL (Whole)': 'Ratio Median'}, inplace = True)\n",
        "      # rename column for better readbility\n",
        "\n",
        "count_df = data_df.groupby(['gene']).count()                    # total number of counts per gene as an independent dataframe\n",
        "count_df.rename(columns = {'Ratios: HLG / ALL (Whole)': 'Count'}, inplace = True)\n",
        "      # rename column for better readbility\n",
        "\n",
        "result_df = data_df.reset_index(drop = True)                       # reset the index of the dataframe\n",
        "result_df = pd.concat([mean_df, median_df, count_df], axis = 1)       \n",
        "      # concatenate the independent dataframes of 'mean', 'median', and 'count' as one large dataframe\n",
        "\n",
        "result_df = result_df.sort_values(by = ['Ratio Median'], ascending = False)\n",
        "      # sort the concatenated dataframe by the median (greatest âŸ¶ least)\n",
        "\n",
        "counter = float(input(\"Enter the count number that you want results that are less than to be cut off at: \")) \n",
        "      # asking user for input, stored as 'counter'\n",
        "      # asking for a cutoff of the total number of counts for a gene\n",
        "greater = result_df[result_df['Count'] < counter].index\n",
        "      # independent dataframe from 'counter', where if the column 'Count' is less than the 'counter', it will be stored in 'greater'\n",
        "\n",
        "result_df.drop(greater, inplace = True)\n",
        "      # remove 'greater' from the concateneated dataframe that is sorted by the median value\n",
        "result_df.reset_index(inplace = True)                        # reset the index of the dataframe\n",
        "\n",
        "groupby = result_df.rename(columns = {'gene' : \"Gene\"})\n",
        "      # new dataframe, using data from the dropped 'greater' and rename the column 'gene' to 'Gene'\n",
        "\n",
        "data_lib = pd.read_csv('large-lib_full_2016.10.25.csv')\n",
        "      # read in data from large-lib_full_2016.10.25.csv, stored in 'data_lib'\n",
        "\n",
        "data_lib = data_lib.drop(['well', 'side', 'chromosome', 'plate', 'strand', \n",
        "                  'min_position', 'full_position', 'orientation', \n",
        "                  'gene_end_distances', 'total_reads', \n",
        "                  'if_both_sides', 'LEAPseq_distance', \n",
        "                  'LEAPseq_percent_confirming',\n",
        "                  'LEAPseq_N_confirming', 'LEAPseq_N_nonconf', \n",
        "                  'LEAPseq_N_unique_pos',\n",
        "                  'flanking_seq', 'plate_errors', 'well_errors', \n",
        "                  'plate_rpm', 'well_rpm',\n",
        "                  'if_fixed_position'], axis = 1)\n",
        "      # drop all â†‘ columns from data_lib\n",
        "\n",
        "level = float(input(\"Enter the confidence level that you want the data to be less than or equal to: \"))\n",
        "      # asking for user input, only accepting numerical values, for the confindence level that you want the resulting data to be less than or equal to, stored in 'level'\n",
        "filter_data_lib = data_lib[data_lib['confidence_level'] <= level]\n",
        "      # begin to filter data, using 'level' store the genes that are less than or equal to the confindence level entered for 'level' in 'filter_data_lib'\n",
        "filter_data_lib = filter_data_lib.sort_values('confidence_level')\n",
        "      # sort filter_data_lib by their confidence levels (least âŸ¶ greatest)\n",
        "filter_data_lib = filter_data_lib.rename(columns = {\"gene\" : \"Gene\",\n",
        "                                          \"confidence_level\" : \"Confidence Level\",\n",
        "                                          \"feature\" : \"Feature\"})\n",
        "      # rename columns: 'gene' to 'Gene', 'confidence_level' to \"Confidence Level\", and 'feature' to \"Feature\" \n",
        "filter_data_lib = filter_data_lib.reset_index(drop = True)\n",
        "      # reset index, and drop old index\n",
        "\n",
        "gene_name = filter_data_lib[\"Gene\"].astype(str).str.replace(r'.v5.5$', '')\n",
        "      # convert data type of column Gene to a string and replace '.v5.5$' with a whitespace\n",
        "gene_name = gene_name.str.replace(r\".v5.5\", '').str.strip()\n",
        "      # remove '.v5.5$'\n",
        "\n",
        "filter_data_lib = filter_data_lib.drop(columns = {\"Gene\"})\n",
        "      # drop Gene column\n",
        "filter_data_lib[\"Gene\"] = gene_name \n",
        "      # replace Gene Column by data where we removed '.v5.5$'\n",
        "\n",
        "filter_data_lib =  filter_data_lib[[\"Gene\", \"Confidence Level\", \"IB\", \"Feature\"]]\n",
        "      # filter data, so that only columns Gene, Confidence Level, IB, and Feature remain and is stored into 'filter_data_lib'\n",
        "\n",
        "df1 = groupby\n",
        "      # dataframe1 (df1) is holding data from groupby, see above â†‘\n",
        "\n",
        "df3 = filter_data_lib\n",
        "      # dataframe3 (df3) is holding data from filter_data_lib, see above â†‘\n",
        "\n",
        "leftBigMerge = pd.merge(left = df1, right = df3, how = 'left', \n",
        "                        left_on = 'Gene', right_on = 'Gene')\n",
        "      # Merge both df1 and df3 to be one dataframe, aligned by gene names, named 'leftBigMerge'\n",
        "      # the 'left' stuff is just so that we don't lose any data when merging, and just in case some data is double counted \n",
        "      # take it like a common factor, but the commonality (gene name) is on the left of the dataframe\n",
        "\n",
        "leftBigMerge = leftBigMerge[[\"Gene\", \"Ratio Median\", \"Ratio Mean\", \n",
        "                             \"Feature\", \"Confidence Level\", \"IB\"]]\n",
        "      # leftBigMerge only to keep columns: Gene, Ratio Median, Ratio Mean, Feature, Confidence Level, and IB\n",
        "\n",
        "# Now the data that we need from 2 dataframes has been consolidated into 1 dataframe, sorted by gene name, organized by ratio median"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdpANp4qaKu4"
      },
      "source": [
        "# Fishing ğŸŸğŸ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp8rwVItPvLQ"
      },
      "source": [
        "**Fisher Exact Test Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1gcMa3vOik3"
      },
      "source": [
        "matrix = leftBigMerge                                     # set 'matrix' to be 'leftBigMerge'\n",
        "\n",
        "oarfish = pd.DataFrame(matrix, columns = [\"Gene\", \"Ratio Median\",\n",
        "                                          \"Ratio Mean\",\n",
        "                                          \"Description\"])\n",
        "      # create another dataframe, named 'oarfish' from data from 'matrix', taking on the columns listed\n",
        "oarfish = oarfish.drop_duplicates()\n",
        "      # drop duplicates from 'oarfish'\n",
        "\n",
        "data = pd.read_csv('TAG Data.csv')\n",
        "      # read in 'TAG Data.csv' and store the data in 'data'\n",
        "\n",
        "data = data[['IB' , 'gene' , 'R5-S#11-HLG_WHOLE_normalized_reads' , 'R5-S#11-ALL_WHOLE_normalized_reads']]\n",
        "      # have data only contain these columns from 'TAG Data.csv'\n",
        "data ['Ratios: HLG / ALL (Whole)'] = data ['R5-S#11-HLG_WHOLE_normalized_reads'] / data['R5-S#11-ALL_WHOLE_normalized_reads']\n",
        "      # create another column for the ratio, high lipid gate vs all\n",
        "\n",
        "data = data.sort_values('gene')                           # sort 'data' by the gene names, alphabetically (numerically in this case)\n",
        "data = data.replace([np.inf, -np.inf], np.nan)            # replace gene names that are 'np.inf' or '-np.inf' to 'np.nan'\n",
        "data = data.dropna()                                      # drop all rows that have 'np.nan' which we coverted in the step above\n",
        "data = data.drop(columns = {\"R5-S#11-HLG_WHOLE_normalized_reads\", \"R5-S#11-ALL_WHOLE_normalized_reads\"})\n",
        "      # drop these columns because the only use for them was to create the ratio, so they are no longer needed\n",
        "\n",
        "ratio = float(input(\"Enter value that you want data to be greater than (Ratio: HLG / ALL (Whole)): \"))\n",
        "      # requesting user input, stored as 'ratio', need to enter a float\n",
        "      # asking for the value of ratio you want the remaining data to be greater than\n",
        "data[\"RatioType\"] = np.where(data[\"Ratios: HLG / ALL (Whole)\"] > ratio, \"Hit\", \"Non-Hit\")\n",
        "      # new column in 'data', entitled, \"RatioType\", where the ratio is greater than the ratio is greater than what was just entered\n",
        "data = data.sort_values(\"gene\")                           # sort 'data' by the gene names, alphabetically (numerically in this case)\n",
        "data = data.reset_index(drop = True)                      # reset the index\n",
        "data = data.replace(['unknown_chrom', 'no_gene_found',   \n",
        "                     'gene_unknown'], np.nan)\n",
        "      # replace values that are 'unknown_chrom', 'no_gene_found', or 'gene_unknown' to NaN\n",
        "data = data.dropna()\n",
        "      # drop all NaN values\n",
        "\n",
        "unknown = data[data.gene.eq(\"unknown_chrom\") |\n",
        "               data.gene.eq(\"no_gene_found\") |\n",
        "               data.gene.eq(\"gene_unknown\")]\n",
        "      # create a dataframe where the gene name is 'unknown_chrom', 'no_gene_found', or 'gene_unknown'\n",
        "      # what we previously dropped\n",
        "\n",
        "unknown = unknown.reset_index(drop = True)\n",
        "      # reset the index of 'unknown'\n",
        "\n",
        "for gene, data_gene in data.groupby('gene'):\n",
        "    data_gene\n",
        "      # consolidates data to its IB, gene name, the ratio, and the ratio type\n",
        "      # runs in a loop until entire dataframe has been searched     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn9r9qHiVmbu"
      },
      "source": [
        "**Actual Fisher Exact Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlmSjxKXQD77"
      },
      "source": [
        "bigHit = 0                                                 # initialize a variable, 'bigHit' and set to 0\n",
        "bigNo = 0                                                  # initialize a varaible, 'bigNo' and set to 0\n",
        "\n",
        "lastGene = data_gene['gene'].iloc[-1]                      # get the very last gene and its information from data_gene, stored as 'lastGene'\n",
        "count = float(input(\"For the total number of counts (hits + non-hits) of a gene, enter a number for cutoff : \"))\n",
        "      # ask user for input, stored as floating 'count'\n",
        "      # asking for the cutoff for genes that have a count that is less than 'count'\n",
        "\n",
        "for gene, data_genie in data.groupby('gene'):               # starting of a loop, that will run and increment until the end of 'data'\n",
        "    data_genie = data_genie.drop(columns = {'IB', \"Ratios: HLG / ALL (Whole)\"})\n",
        "      # 'data_genie' is a dataframe that takes in every gene, one by one, per increment, and only keeps gene name, and ratio type \n",
        "   \n",
        "    hit = data_genie.loc[data_genie.RatioType == 'Hit', \"RatioType\"].count()\n",
        "      # hit is a dataframe that holds the genes with the ratio type, \"Hit\", and counts it\n",
        "    noHit = data_genie.loc[data_genie.RatioType == \"Non-Hit\", \"RatioType\"].count()\n",
        "      # noHit is a dataframe that holds the genes with the ratio type, \"Non-Hit\", and counts it\n",
        "\n",
        "    bigHit =  bigHit + hit                                  # count the number of hits, adding bigHit on top of bigHit, which is why bigHit was initialized outside of the loop\n",
        "    bigNo = bigNo + noHit                                   # count the number of non-hits, adding bigNo on top of bigNo, which is why bigNo was initialized outside of the loop\n",
        "\n",
        "    if lastGene in data_genie.values:                       # if the lastGene is the current iteration, the loop ends, else, the current gene is not the lastGene, the loop continues to run\n",
        "        sum = pd.DataFrame (                                # dataframe, 'sum', stores the total number of hits and total number of non-hits\n",
        "            { \n",
        "                'Hit' : bigHit,                             # make a column, in 'sum', 'Hit' with the data from bigHit \n",
        "                'Non-Hit' : bigNo                           # make a column, in 'sum', 'Non-Hit' with the data from bigNo\n",
        "            }, index = [0]                                \n",
        "        )                                                   # end of if statement\n",
        "\n",
        "# end of first for loop, now we have a dataframe with just the gene name (with their corresponsding ratio type), and the total number of hits and non-hits of all genes\n",
        "\n",
        "goaway = pd.DataFrame()                                     # initialize empty dataframe, 'goaway'\n",
        "biggergoaway = pd.DataFrame()                               # initialize empty dataframe, 'biggergoaway'\n",
        "\n",
        "goname = pd.DataFrame()                                     # initialize empty dataframe, 'goname'\n",
        "biggergoname = pd.DataFrame()                               # initialize empty dataframe, 'biggergoname'\n",
        "\n",
        "for gene, data_genie in data.groupby('gene'):               # second for loop, will run and increment until the end of 'data'\n",
        "    data_genie = data_genie.drop(columns = {'IB', \"Ratios: HLG / ALL (Whole)\"})\n",
        "      # 'data_genie' is a dataframe that takes in every gene, one by one, per increment, and only keeps gene name, and ratio type\n",
        "   \n",
        "    hit = data_genie.loc[data_genie.RatioType == 'Hit', \"RatioType\"].count()\n",
        "      # hit is a dataframe that holds the genes with the ratio type, \"Hit\", and counts it\n",
        "    noHit = data_genie.loc[data_genie.RatioType == \"Non-Hit\", \"RatioType\"].count()\n",
        "      # noHit is a dataframe that holds the genes with the ratio type, \"Non-Hit\", and counts it\n",
        "    \n",
        "    bigHit =  bigHit + hit                                  # count the number of hits, adding bigHit on top of bigHit, which is why bigHit was initialized outside of the loop\n",
        "    bigNo = bigNo + noHit                                   # count the number of non-hits, adding bigNo on top of bigNo, which is why bigNo was initialized outside of the loop\n",
        "    \n",
        "    genius = pd.DataFrame (                                 # create a new dataframe, 'genius', with one column, 'Gene', storing each unique gene\n",
        "        {\n",
        "        'Gene': data_genie.gene.unique()\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    frame = pd.DataFrame (                                  # create a new dataframe, 'frame', for each gene and their hits and non-hits\n",
        "         {\n",
        "            'Hit' : hit,\n",
        "            'Non-Hit' : noHit\n",
        "        }, index = [0]\n",
        "    )\n",
        "\n",
        "    concat = pd.concat([frame, sum], ignore_index = True)   # concatenate the dataframes 'frame' and 'sum' (individual gene hits and non-hits vs. total gene hits and non-hits) to 'concat'\n",
        "    concat = concat.to_numpy()                              # convert 'concat' to all numpy values\n",
        "    \n",
        "    obs = np.array(concat)                                  # 'obs' takes on 'concat' as an array of numpy values instead of a dataframe\n",
        "    new = pd.DataFrame(obs)                                 # 'new' takes on 'obs' as a dataframe\n",
        "\n",
        "# beginning of the Fisher Exact Test, everything up until now was in preparation\n",
        "    oddsratio, fisher = stats.fisher_exact(new)             # performs Fisher Exact Test on a 2 x 2 contingency table\n",
        "        # contingency table:                                # store the p-values as 'fisher'\n",
        "        # |        |   Hit   |   Non-Hit   |\n",
        "        # |â€”â€”â€”â€”â€”â€”â€”â€”|â€”â€”â€”â€”â€”â€”â€”â€”â€”|â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”|  \n",
        "        # |  gene  |         |             | â† (this is from 'frame')\n",
        "        # |â€”â€”â€”â€”â€”â€”â€”â€”|â€”â€”â€”â€”â€”â€”â€”â€”â€”|â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”|  \n",
        "        # |  total |         |             | â† (this is from 'sum')\n",
        "        # this is what was concatenated a few lines above\n",
        "    \n",
        "    fishy = np.asarray(fisher)                              # convert 'fisher' to an array, stored as 'fishy' \n",
        "    fishy = fishy.reshape(-1, 1)                            \n",
        "        # reshape p-values in 'fishy' to be their own arrays:\n",
        "        # array ([[ p-value of the first gene],\n",
        "        #         [ p-value of the second gene],\n",
        "        #         [ p-value of the third gene], \n",
        "        #         etc.])                           \n",
        "\n",
        "    frame.astype(float)                                     # ensure that 'frame' stores floats\n",
        "    sum.astype(float)                                       # ensure that 'sum' stores floats\n",
        "\n",
        "    fsh = pd.DataFrame(data = fishy, columns = [\"p-value\"])\n",
        "        # 'fsh' is a dataframe storing the data from 'fishy' under a column \"p-values\"\n",
        "    fsh.astype(float)                                       # make sure 'fsh' stores floats\n",
        "    \n",
        "    oarfish = np.concatenate((frame, sum), axis = 0)        # concatenate 'frame' and 'sum', store as 'oarfish'\n",
        "    oarfish = pd.DataFrame(data = oarfish, columns = [\"Hit\", \"Non-Hit\"])\n",
        "        # for the data in 'oarfish', title the two columns \"Hit\" and \"Non-Hit\"\n",
        "    anchovy = pd.DataFrame(data = fsh, columns = [\"p-value\"])\n",
        "        # data in 'fsh' is the p-values, stored as a dataframe, 'anchovy'\n",
        "\n",
        "        # â†“ if I recall correctly, this is what is made for every gene â†“\n",
        "        # | Gene Name                      |\n",
        "        # |â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”|\n",
        "        # |        |   Hit   |   Non-Hit   |\n",
        "        # |â€”â€”â€”â€”â€”â€”â€”â€”|â€”â€”â€”â€”â€”â€”â€”â€”â€”|â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”|\n",
        "        # |  gene  |         |             |\n",
        "        # |â€”â€”â€”â€”â€”â€”â€”â€”|â€”â€”â€”â€”â€”â€”â€”â€”â€”|â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”|  \n",
        "        # |  total |         |             |\n",
        "        # |â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”|\n",
        "        # |        |         |     p-value |\n",
        "\n",
        "    bigFish = pd.concat([oarfish, anchovy], ignore_index = True, sort = True)\n",
        "        # concatenate 'oarfish' and 'anchovy', stored as 'bigFish'\n",
        "    bigFish = pd.concat([genius, bigFish], axis = 1)\n",
        "        # update 'bigFish' by storing the concatenation of 'genius' (which was the unique gene names) \n",
        "        # and 'bigFish' (from the step above, is the concatenation of 'oarfish' and 'anchovy')\n",
        "\n",
        "    bigFish.loc[bigFish['Hit'] + bigFish['Non-Hit'] < count] = \"Go Away\"\n",
        "        # taking the user input, from the beginning of the cell, asking for the count that you want \n",
        "        # the remaining data to be greater than, from 'bigFish', replace the values \n",
        "        # in the gene name column with \"Go Away\"\n",
        "\n",
        "    if bigFish.iloc[0, 0] == \"Go Away\":                       # if the gene name is exactly \"Go Away\"\n",
        "      bigFish.iloc[2, 3] = \"NO\"                               # then \"NO\" is stored in the p-value's cell of the dataframe\n",
        "\n",
        "    goaway = bigFish.iloc[2]                                  # 'goaway' stores the data from 'bigFish' if it is \"Go Away\"    \n",
        "    biggergoaway = biggergoaway.append(goaway)                # 'biggergoaway' is used to append 'goway' to each other \n",
        "\n",
        "    goname = bigFish.iloc[0]                                  # 'goname' stores names from 'bigFish'\n",
        "    biggergoname = biggergoname.append(goname)                # 'biggergoname' is used to append 'goname' to each other\n",
        "\n",
        "# this all happens in a for loop, so the resulting dataframes are larger and larger\n",
        "# please forgive the long run time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4eeDbGBVcEl"
      },
      "source": [
        "# taking the last dataframe from the last iteration of the for-loop in the previous cell:\n",
        "away = biggergoaway.drop(columns = [\"Gene\", \"Hit\", \"Non-Hit\"]) \n",
        "      # 'away' stores the data from 'biggergoaway', dropping the \"Gene\", \"Hit\", and \"Non-Hit\" columns\n",
        "      # only \"p-values\" remains\n",
        "away = away[away[\"p-value\"] != \"NO\"]\n",
        "      # 'away' is updated as it checks all the values in \"p-value\", keeping only those that are not \"NO\"\n",
        "away = away.reset_index(drop = True)\n",
        "      # reset the index of 'away'\n",
        "\n",
        "pen = biggergoname.drop(columns = [\"Hit\", \"Non-Hit\", \"p-value\"])\n",
        "      # 'pen' stores the data from 'biggergoname', dropping the \"Hit\", \"Non-Hit\", and \"p-value\" columns\n",
        "      # only \"Gene\" remains\n",
        "pen = pen[pen[\"Gene\"] != \"Go Away\"]\n",
        "      # 'pen' is updated as it checks all the values in \"Gene\", keeping only those that are not \"Go Away\"\n",
        "pen = pen.reset_index(drop = True)\n",
        "      # reset the index of pen\n",
        "\n",
        "pen_away = pen.merge(away, left_index = True, right_index = True)\n",
        "      # merge 'pen' and 'away' by the gene name, stored as 'pen_away'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rez3QbLQBXE"
      },
      "source": [
        "#Benjamini-Hochberg Adjustment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLeKkaSVVwsF"
      },
      "source": [
        "penguin = pen_away[~pen_away.Gene.str.contains(\" & \")]\n",
        "      # drop any rows in 'pen_away' with values in the \"Gene\" column that contain \" & \"\n",
        "      # store the remaining rows in 'penguin'\n",
        "penguin = penguin.reset_index(drop = True)                # reset the index of 'penguin'\n",
        "\n",
        "bh_num = float(input(\"Enter the value that all adjusted p-values should be less than (0-1): \"))\n",
        "      # user input, asking for the maximum of the adjusted p-value, store as 'bh_num'\n",
        "\n",
        "# â†“ Benjamini-Hochberg Adjustment of p-values â†“\n",
        "# calls on a statisical adjustment from an R library: \n",
        "def FDR_adjust_pvalues(case, N=None, method='BH'):\n",
        "    \"\"\" Adjust a list of p-values for false discovery rate using R's stats::p.adjust function.\n",
        "\n",
        "    N and method are passed to R_stats.p_adjust:\n",
        "     - N is the number of comparisons (if left unspecified, defaults to len(pvalue_list), I think)\n",
        "     - method is the name of the adjustment method to use (inherited from R)\n",
        "\n",
        "    Note that this MUST be done after all the p-values are already collected, on the full list of p-values at once:\n",
        "     trying to do it on single p-values, even with adjusted N, will give different results!\n",
        "    \"\"\"\n",
        "    if not method in R_stats.p_adjust_methods:\n",
        "        raise ValueError(\"Unknown method %s - method must be one of (%s)!\"%(method, ', '.join(R_stats.p_adjust_methods)))\n",
        "    if N is None:   return R_stats.p_adjust(FloatVector(case), method=method)\n",
        "    else:           return R_stats.p_adjust(FloatVector(case), method=method, n=N)\n",
        "\n",
        "penguin[\"Adjusted p-value\"] = FDR_adjust_pvalues(penguin['p-value'])\n",
        "      # make a new column in 'penguin', \"Adjusted p-value\"\n",
        "      # using False Discovery Rates (FDR) / correction on the adjusted p-values\n",
        "\n",
        "penguin.sort_values(by = 'p-value')\n",
        "      # sort 'penguin' by their p-values\n",
        "\n",
        "adelie = penguin.loc[penguin[\"Adjusted p-value\"] < bh_num]\n",
        "      # keeps the rows with adjusted p-values that are less than 'bh_num', stored as dataframe 'adelie'\n",
        "\n",
        "display(adelie)                                             # display 'adelie' as a dataframe\n",
        "\n",
        "# conclusion: total time to run all cells 8 - 10 minutes\n",
        "# majority of the time is taken by the \"Actual Fisher Test\" cell"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}